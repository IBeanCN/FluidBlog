<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>大数据-Spark篇-面试题总结 | 野生Java程序员兼美食博主</title><meta name="author" content="IBean"><meta name="copyright" content="IBean"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="题目来源 最全腾讯等BAT大数据面试99题 以下答案仅供参考，如有错误请指正 1. Spark的Shuffle原理及调优？ 原理：SparkShuffle过程与MapReduce类似；DAG阶段，以Shuffle为界分为map stage和reduce stage，map阶段进行任务计算后下发到各自partition中，同时写入磁盘，该过程为shuffle write；reduce阶段读取map计">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据-Spark篇-面试题总结">
<meta property="og:url" content="https://blog.ibean.eu.org/posts/45903.html">
<meta property="og:site_name" content="野生Java程序员兼美食博主">
<meta property="og:description" content="题目来源 最全腾讯等BAT大数据面试99题 以下答案仅供参考，如有错误请指正 1. Spark的Shuffle原理及调优？ 原理：SparkShuffle过程与MapReduce类似；DAG阶段，以Shuffle为界分为map stage和reduce stage，map阶段进行任务计算后下发到各自partition中，同时写入磁盘，该过程为shuffle write；reduce阶段读取map计">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2020-02-08T09:52:35.000Z">
<meta property="article:modified_time" content="2024-01-26T08:29:33.944Z">
<meta property="article:author" content="IBean">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="BigData">
<meta property="article:tag" content="面试">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://blog.ibean.eu.org/posts/45903.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//static.cloudflareinsights.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="GwErIolW7GpgkehH5VTnqoiTqYAFTOceiJlgJt7exug"/><meta name="baidu-site-verification" content="codeva-cPxroj7IpB"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: 'ca-pub-5097109782941887',
  enable_page_level_ads: 'true'
});</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-Q0KPP9PVCD"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-Q0KPP9PVCD');
</script><script defer="defer" data-pjax="data-pjax" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;e1adc92acf514789ac39ddcec3c1473c&quot;}"></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据-Spark篇-面试题总结',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-01-26 16:29:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="野生Java程序员兼美食博主" type="application/atom+xml">
<link rel="alternate" href="/rss2.xml" title="野生Java程序员兼美食博主" type="application/rss+xml">
</head><body><link rel="stylesheet" href="/css/corner-indicator.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/heart/"><i class="fa-fw fas fa-heart"></i><span> Heart</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/bg.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="野生Java程序员兼美食博主"><span class="site-name">野生Java程序员兼美食博主</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/heart/"><i class="fa-fw fas fa-heart"></i><span> Heart</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据-Spark篇-面试题总结</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-02-08T09:52:35.000Z" title="发表于 2020-02-08 17:52:35">2020-02-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-01-26T08:29:33.944Z" title="更新于 2024-01-26 16:29:33">2024-01-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据-Spark篇-面试题总结"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong>题目来源</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.xuehua.us/2018/08/23/%E6%9C%80%E5%85%A8%E8%85%BE%E8%AE%AF%E7%AD%89bat%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%9599%E9%A2%98%EF%BC%9Ahadoop%E3%80%81java%E3%80%81spark%E3%80%81%E6%9C%BA%E5%99%A8%E7%AE%97%E6%B3%95%E7%AD%89/">最全腾讯等BAT大数据面试99题</a> <strong>以下答案仅供参考，如有错误请指正</strong></p>
<h4 id="1-Spark的Shuffle原理及调优？"><a href="#1-Spark的Shuffle原理及调优？" class="headerlink" title="1. Spark的Shuffle原理及调优？"></a><strong>1. Spark的Shuffle原理及调优？</strong></h4><ul>
<li><strong>原理</strong>：<br>SparkShuffle过程与MapReduce类似；DAG阶段，以Shuffle为界分为map stage和reduce stage，map阶段进行任务计算后下发到各自partition中，同时写入磁盘，该过程为shuffle write；reduce阶段读取map计算后各分区的值进一步计算，该过程为shuffle read；</li>
<li><strong>调优：</strong><ol>
<li>尽量减少shuffle次数</li>
<li>必要时主动shuffle，通常用于改变并行度，提高后续分布式运行速度</li>
<li>合并Map端输出文件<br>   开启相关设置：<code>conf.set(“spark.shuffle.consolidateFiles”,”  true”)</code></li>
<li>调节map端内存缓冲区大小和reduce端内存缓冲区内存占比<ul>
<li><code>spark.shuffle.file.buffer</code>，默认32k</li>
</ul>
<p> 在map task处理的数据量比较大的情况下，而你的task的内存缓冲默认是比较小的，32kb。可能会造成多次的map端往磁盘文件的spill溢写操作，发生大量的磁盘IO，从而降低性能。<br> 调优原则：<br> <code>spark.shuffle.file.buffer</code>，每次扩大一倍，然后看看效果，64，128；</p>
<ul>
<li><code>spark.shuffle.memoryFraction</code>，0.2</li>
</ul>
<p> reduce端聚合内存，占比。默认是0.2。如果数据量比较大，reduce task拉取过来的数据很多，那么就会频繁发生reduce端聚合内存不够用，频繁发生spill操作，溢写到磁盘上去。而且最要命的是，磁盘上溢写的数据量越大，后面在进行聚合操作的时候，很可能会多次读取磁盘中的数据，进行聚合。<br> 调优原则：<br> <code>spark.shuffle.memoryFraction</code>，每次提高0.1，看看效果。</p>
</li>
<li>SortShuffle代替HashShuffle，避免创建多分磁盘文件（Spark2.0之后的版本官方仅保留了SortShuffle）</li>
</ol>
</li>
</ul>
<p><em><strong>参考连接：</strong><a target="_blank" rel="noopener external nofollow noreferrer" href="http://sharkdtu.com/posts/spark-shuffle.html">Spark Shuffle原理及相关调优</a></em></p>
<hr>
<h4 id="2-hadoop和spark使用场景？"><a href="#2-hadoop和spark使用场景？" class="headerlink" title="2. hadoop和spark使用场景？"></a><strong>2. hadoop和spark使用场景？</strong></h4><ul>
<li><strong>Hadoop</strong><br>离线大数据量批量计算，实时要求性低场景，大数据量存储，日志处理，个性化广告推荐</li>
<li><strong>Spark</strong><br>内存分布式计算框架，伪实时计算，微批计算达到实时，产品实时推荐</li>
</ul>
<hr>
<h4 id="3-spark如何保证宕机迅速恢复"><a href="#3-spark如何保证宕机迅速恢复" class="headerlink" title="3. spark如何保证宕机迅速恢复?"></a><strong>3. spark如何保证宕机迅速恢复?</strong></h4><ul>
<li>适当增加Spark的StandBy Master；</li>
<li>编写脚本定期给Master发心跳，若宕机则进行重启；</li>
</ul>
<hr>
<h4 id="4-hadoop和spark的相同点和不同点？"><a href="#4-hadoop和spark的相同点和不同点？" class="headerlink" title="4. hadoop和spark的相同点和不同点？"></a><strong>4. hadoop和spark的相同点和不同点？</strong></h4><ul>
<li><p><strong>Hadoop</strong><br>Hadoop的MR为多进程模型，系统稳定性优于Spark，但MR只有Map和Reduce两个阶段，导致表达能力欠缺，操作仅能在这两个阶段完成；同时读写HDFS时会产生大量磁盘IO操作；Hadoop适合于大数据量批量的或者高延迟的离线数据计算；</p>
</li>
<li><p><strong>Spark</strong><br>Spark为基于内存的多线程模型，系统稳定性略差，但是计算速度优于Hadoop，同时因为Spark基于内存进行计算，在不对系统进行调优的情况下很容易出现OOM等系统问题，导致任务结束无法继续；提供丰富的算子转换操作；使数据处理不单单拘泥于Map和Reduce，计算模型更加灵活；Spark适合低延迟内存密集型的数据计算；</p>
</li>
</ul>
<hr>
<h4 id="5-RDD持久化原理？"><a href="#5-RDD持久化原理？" class="headerlink" title="5. RDD持久化原理？"></a><strong>5. RDD持久化原理？</strong></h4><p>通过调用cache()和persist()方法将计算后的数据持久化到内存中；<br>其中cache()的默认实现即为persist(MEMORY_ONLY)；<br>persist()有可选项：</p>
<ul>
<li><code>MEMORY_ONLY</code>：仅存于内存，无法存储的partition会被重新计算；</li>
<li><code>MEMORY_AND_DISK</code>：存于内存和磁盘，无法存储的partition会被写入磁盘，需要时从磁盘读取</li>
<li><code>MEMORY_ONLY_SER</code>：仅存于内存，但会序列化；序列化减少内存开销，但取用时反序列化会导致CPU占用过高</li>
<li><code>MEMORY_AND_DISK_SER</code>：同 <code>MEMORY_AND_DISK</code>，但会被序列化</li>
<li><code>DISK_ONLY</code>：仅存于磁盘；</li>
<li><code>MEMORY_ONLY_2/MEMERY_AND_DISK_2</code>：尾部加2表示会进行备份，防止数据丢失</li>
</ul>
<hr>
<h4 id="6-checkpoint检查点机制？"><a href="#6-checkpoint检查点机制？" class="headerlink" title="6. checkpoint检查点机制？"></a><strong>6. checkpoint检查点机制？</strong></h4><p>SparkStreaming中的容错保证机制，保证程序出错时，仍可以从检查点恢复，保证数据高可用，同时可以减少回溯时间尽快重新开始计算</p>
<hr>
<h4 id="7-checkpoint和持久化机制的区别？"><a href="#7-checkpoint和持久化机制的区别？" class="headerlink" title="7. checkpoint和持久化机制的区别？"></a><strong>7. checkpoint和持久化机制的区别？</strong></h4><p>持久化是将数据保存在磁盘中，RDD的lineage仍旧保持不变，但持久化更容易丢数据，节点故障会导致数据从磁盘和内存中消失；检查点机制会将数据保存到高可用的文件系统中，与此同时lineage发生改变，检查点之前的lineage消失，只有当前点的状态；</p>
<hr>
<h4 id="8-Spark-Streaming和Storm有何区别？"><a href="#8-Spark-Streaming和Storm有何区别？" class="headerlink" title="8. Spark Streaming和Storm有何区别？"></a><strong>8. Spark Streaming和Storm有何区别？</strong></h4><ul>
<li>SparkStreaming 秒级实时计算；RDD 本质是小批量的计算达到实时；但也因此吞吐量高于Storm</li>
<li>Storm 毫秒级实时计算；数据收到即处理；可以实时调整并行度；<br>所以：对实时性要求高或者数据量存在不稳定的情况下可选Storm，峰值动态调整并行度；对实时性要求一般可选SparkStreaming</li>
</ul>
<hr>
<h4 id="9-RDD机制？"><a href="#9-RDD机制？" class="headerlink" title="9. RDD机制？"></a><strong>9. RDD机制？</strong></h4><p>RDD弹性分布式数据集，一种数据结构，所有算子基于RDD执行；RDD具有很好的容错性，当节点错误partition数据丢失，可通过lineage的关系回溯重新计算partition的值；RDD的弹性体现在于RDD上自动进行内存和磁盘之间权衡和切换的机制。</p>
<hr>
<h4 id="10-Spark-streaming以及基本工作原理？"><a href="#10-Spark-streaming以及基本工作原理？" class="headerlink" title="10. Spark streaming以及基本工作原理？"></a><strong>10. Spark streaming以及基本工作原理？</strong></h4><p>SparkStreaming是一个秒级实时的，高吞吐量，利用多种算子和转换完成数据处理的流失批处理框架；<br>streaming接收数据后，将数据划分为一个个的batch交给spark引擎处理，处理后汇总成一个数据流，其中数据依然是一个个batch组成的；</p>
<hr>
<h4 id="11-DStream以及基本工作原理？"><a href="#11-DStream以及基本工作原理？" class="headerlink" title="11. DStream以及基本工作原理？"></a><strong>11. DStream以及基本工作原理？</strong></h4><p>Dstream是一个抽象概念，代表一个持续不断的数据源，可由Kafka，flume等主动生成，也可由map，reduce等算子被动生成；<br>Dstream内部不断产生RDD，RDD为一个batch，每个RDD中包含一个时间段的数据；</p>
<hr>
<h4 id="12-spark有哪些组件？"><a href="#12-spark有哪些组件？" class="headerlink" title="12. spark有哪些组件？"></a><strong>12. spark有哪些组件？</strong></h4><p>Master管理集群；<br>worker计算节点；<br>Driver运行main函数；<br>Spark Context管理Spark生命周期，Client用户提交程序</p>
<hr>
<h4 id="13-spark工作机制？"><a href="#13-spark工作机制？" class="headerlink" title="13. spark工作机制？"></a><strong>13. spark工作机制？</strong></h4><p>用户Client端提交程序，Driver运行程序main方法，生成Spark Context上下文；生成dag图，根据rdd间依赖关系划分task，将task提交到executor中执行；</p>
<hr>
<h4 id="14-Spark工作的一个流程？"><a href="#14-Spark工作的一个流程？" class="headerlink" title="14. Spark工作的一个流程？"></a><strong>14. Spark工作的一个流程？</strong></h4><p><img src="/../img/webp.webp" alt="spark工作流程图"></p>
<ul>
<li>构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；</li>
<li>资源管理器分配Executor资源并启动Executor，Executor运行情况将随着心跳发送到资源管理器上；</li>
<li>SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor。</li>
<li>Task在Executor上运行，运行完毕释放所有资源。</li>
</ul>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/01599e28090d">Spark 工作流程图</a></em></p>
<hr>
<h4 id="15-spark核心编程原理？"><a href="#15-spark核心编程原理？" class="headerlink" title="15. spark核心编程原理？"></a><strong>15. spark核心编程原理？</strong></h4><ul>
<li>定义初始RDD来源，可以为本地文件，HDFS或者输入流；</li>
<li>对RDD进行计算，通过算子转换等操作</li>
<li>循环往复程，第一个计算完了以后，数据可能就会到了新的一批节点上，也就是变成一个新的RDD。然后再次反复，针对新的RDD定义计算操作</li>
<li>获得最终数据保存数据</li>
</ul>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/bbaiggey/article/details/51243061">Spark核心编程原理</a></em></p>
<hr>
<h4 id="16-spark基本工作原理？"><a href="#16-spark基本工作原理？" class="headerlink" title="16. spark基本工作原理？"></a><strong>16. spark基本工作原理？</strong></h4><p>首先在本地客户端(client)编写spark程序，然后将程序打成jar包，在某台能够连接到spark集群的机器上提交spark程序，spark程序会被提交到spark集群上运行。</p>
<hr>
<h4 id="17-spark性能优化有哪些？"><a href="#17-spark性能优化有哪些？" class="headerlink" title="17. spark性能优化有哪些？"></a><strong>17. spark性能优化有哪些？</strong></h4><p>分配更多资源；<br>调整并行度；<br>重构RDD架构以及RDD持久化；<br>广播大变量；<br>使用Kryo序列化；<br>使用fastutil优化数据格式；<br>调节数据本地化等待时长；</p>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://mubu.com/doc/iK9RFqMfd0">Spark性能调优</a></em> <strong>密码：ibean.top，如果分享关闭请留言</strong></p>
<hr>
<h4 id="18-updateStateByKey详解？"><a href="#18-updateStateByKey详解？" class="headerlink" title="18. updateStateByKey详解？"></a><strong>18. updateStateByKey详解？</strong></h4><ul>
<li>大数量updateBykey不适合，可以采用redis</li>
<li>key超时，如何清空，来节约内存<br> 由于已存在状态的key，无论是否在新批次里有数据，都会调用updateFunc。<br> 返回None就可以清空超时key</li>
<li>初始状态<br> 对于状态的算子一定要开启checkpoint，实际就是指定checkpoint目录<br> checkpoint频率：5-10个滑动窗口</li>
</ul>
<p><em><strong>参考链接</strong><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://mp.weixin.qq.com/s/jQIBvKnNSqRJIAF6Eqcdtg">sparkstreaming状态管理upstatebykey</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://mp.weixin.qq.com/s/AGdTlglZss7AEtajdsUk6w">sparkstreaming状态管理外部存储篇</a></em></p>
<hr>
<h4 id="19-宽依赖和窄依赖？"><a href="#19-宽依赖和窄依赖？" class="headerlink" title="19. 宽依赖和窄依赖？"></a><strong>19. 宽依赖和窄依赖？</strong></h4><ul>
<li>窄依赖就是指父RDD的每个分区只被一个子RDD分区使用<br>  <img src="/../img/webp-167340514296860.webp" alt="窄依赖"></li>
<li>宽依赖就是指父RDD的每个分区都有可能被多个子RDD分区使用<br>  <img src="/../img/webp-167340514296861.webp" alt="宽依赖"></li>
</ul>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/5c2301dfa360">Spark宽依赖与窄依赖</a></em></p>
<hr>
<h4 id="20-spark-streaming中有状态转化操作？"><a href="#20-spark-streaming中有状态转化操作？" class="headerlink" title="20. spark streaming中有状态转化操作？"></a><strong>20. spark streaming中有状态转化操作？</strong></h4><p><strong>有状态转化：</strong> 依赖之前的批次数据或者中间结果来计算当前批次的数据，包括<strong>updateStatebyKey()和window()</strong></p>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/031dbd1fa2a7">SparkStreaming入门教程(四)有状态和无状态的转化操作</a></em></p>
<hr>
<h4 id="21-spark常用的计算框架？"><a href="#21-spark常用的计算框架？" class="headerlink" title="21. spark常用的计算框架？"></a><strong>21. spark常用的计算框架？</strong></h4><p>Spark Core用于离线计算，Spark SQL用于交互式查询，Spark Streaming用于实时流式计算，Spark MLlib用于机器学习，Spark GraphX用于图计算。<br>Spark主要用于大数据的计算，而hadoop主要用于大数据的存储(比如hdfs、hive和hbase等)，以及资源调度yarn。Spark+hadoop的组合是未来大数据领域的热门组合</p>
<hr>
<h4 id="22-spark整体架构？"><a href="#22-spark整体架构？" class="headerlink" title="22. spark整体架构？"></a><strong><del>22. spark整体架构？</del></strong></h4><hr>
<h4 id="23-Spark的特点是什么？"><a href="#23-Spark的特点是什么？" class="headerlink" title="23. Spark的特点是什么？"></a><strong>23. Spark的特点是什么？</strong></h4><p>(1)速度快：Spark基于内存进行计算（当然也有部分计算基于磁盘，比如shuffle）。<br>(2)容易上手开发：Spark的基于RDD的计算模型，比Hadoop的基于Map-Reduce的计算模型要更加易于理解，更加易于上手开发，实现各种复杂功能，比如二次排序、topn等复杂操作时，更加便捷。<br>(3)超强的通用性：Spark提供了Spark RDD、Spark SQL、Spark Streaming、Spark MLlib、Spark GraphX等技术组件，可以一站式地完成大数据领域的离线批处理、交互式查询、流式计算、机器学习、图计算等常见的任务。<br>(4)集成Hadoop：Spark并不是要成为一个大数据领域的“独裁者”，一个人霸占大数据领域所有的“地盘”，而是与Hadoop进行了高度的集成，两者可以完美的配合使用。Hadoop的HDFS、Hive、HBase负责存储，YARN负责资源调度；Spark复杂大数据计算。实际上，Hadoop+Spark的组合，是一种“double win”的组合。<br>(5)极高的活跃度：Spark目前是Apache基金会的顶级项目，全世界有大量的优秀工程师是Spark的committer。并且世界上很多顶级的IT公司都在大规模地使用Spark。</p>
<hr>
<h4 id="24-搭建spark集群步骤？"><a href="#24-搭建spark集群步骤？" class="headerlink" title="24. 搭建spark集群步骤？"></a><strong>24. 搭建spark集群步骤？</strong></h4><ul>
<li>安装spark包</li>
<li>修改 spark-env. sh</li>
<li>修改slaves文件</li>
<li>分发spark包，安装spark集群</li>
<li>启动spark集群</li>
<li>查看集群状态，spark集群的默认web管理页面端口为8080，url为<a target="_blank" rel="noopener external nofollow noreferrer" href="http://master:8080/">http://master:8080</a></li>
</ul>
<hr>
<h4 id="25-Spark的三种提交模式是什么？"><a href="#25-Spark的三种提交模式是什么？" class="headerlink" title="25. Spark的三种提交模式是什么？"></a><strong>25. Spark的三种提交模式是什么？</strong></h4><ul>
<li>Spark内核架构，即standalone模式，基于Spark自己的Master-Worker集群；</li>
<li>基于Yarn的yarn-cluster模式；</li>
<li>基于Yarn的yarn-client模式。</li>
</ul>
<p>如果要切换到第二种和第三种模式，将之前提交spark应用程序的spark-submit脚本，加上–master参数，设置为yarn-cluster，或yarn-client即可。如果没设置就是standalone模式</p>
<hr>
<h4 id="26-spark内核架构原理？"><a href="#26-spark内核架构原理？" class="headerlink" title="26. spark内核架构原理？"></a><strong><del>26. spark内核架构原理？</del></strong></h4><hr>
<h4 id="27-Spark-yarn-cluster架构？"><a href="#27-Spark-yarn-cluster架构？" class="headerlink" title="27. Spark yarn-cluster架构？"></a><strong>27. Spark yarn-cluster架构？</strong></h4><p>Yarn-cluster用于生产环境，优点在于driver运行在NM，没有网卡流量激增的问题。缺点在于调试不方便，本地用spark-submit提交后，看不到log，只能通过yarm application-logs application_id这种命令来查看，很麻烦。<br>(1)将spark程序通过spark-submit命令提交，会发送请求到RM(相当于Master)，请求启动AM；<br>(2)在yarn集群上，RM会分配一个container，在某个NM上启动AM；<br>(3)在NM上会启动AM(相当于Driver)，AM会找RM请求container，启动executor；<br>(4)RM会分配一批container用于启动executor；<br>(5)AM会连接其他NM(相当于worker)，来启动executor；<br>(6)executor启动后，会反向注册到AM。</p>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/61902619">Spark on Yarn两种模式剖析</a></em></p>
<hr>
<h4 id="28-Spark-yarn-client架构？"><a href="#28-Spark-yarn-client架构？" class="headerlink" title="28. Spark yarn-client架构？"></a><strong>28. Spark yarn-client架构？</strong></h4><p>Yarn-client用于测试，因为driver运行在本地客户端，负责调度application，会与yarn集群产生大量的网络通信，从而导致网卡流量激增，可能会被公司的SA警告。好处在于，直接执行时本地可以看到所有的log，方便调试。<br>(1)将spark程序通过spark-submit命令提交，会发送请求到RM，请求启动AM；<br>(2)在yarn集群上，RM会分配一个container在某个NM上启动application；<br>(3)在NM上会启动application master，但是这里的AM其实只是一个ExecutorLauncher，功能很有限，只会去申请资源。AM会找RM申请container，启动executor；<br>(4)RM会分配一批container用于启动executor；<br>(5)AM会连接其他NM(相当于worker)，用container的资源来启动executor；<br>(6)executor启动后，会反向注册到本地的Driver进程。通过本地的Driver去执行DAGsheduler和Taskscheduler等资源调度。<br>和Spark yarn-cluster的区别在于，cluster模式会在某一个NM上启动AM作为Driver。</p>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/61902619">Spark on Yarn两种模式剖析</a></em></p>
<hr>
<h4 id="29-SparkContext初始化原理？"><a href="#29-SparkContext初始化原理？" class="headerlink" title="29. SparkContext初始化原理？"></a><strong>29. SparkContext初始化原理？</strong></h4><ul>
<li>TaskScheduler如何注册application，executor如何反向注册到TaskScheduler；</li>
<li>DAGScheduler；</li>
<li>SparkUI。</li>
</ul>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/8e73d08219bd">sparkContext原理剖析</a></em></p>
<hr>
<h4 id="30-Spark主备切换机制原理剖析？"><a href="#30-Spark主备切换机制原理剖析？" class="headerlink" title="30. Spark主备切换机制原理剖析？"></a><strong>30. Spark主备切换机制原理剖析？</strong></h4><ul>
<li>spark master的主备切换可以基于两种机制，一种是基于文件系统的，一种是基于zookeeper的<ul>
<li>基于文件系统的主备切换机制在active master挂掉之后，需要我们手动去切换到standby master；</li>
<li>而基于zookeeper的主备切换机制在active master挂掉之后，可以实现自动的切换到standby master。</li>
</ul>
</li>
<li>这里要说的master主备切换机制就是，在在active master挂掉之后切换到standby master，master会做哪些操作。</li>
</ul>
<p><em><strong>参考链接</strong><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/4e1b2d986883">spark主备切换机制剖析</a>、<br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/c5f51b68a639">spark的主备切换极致原理剖析</a></em></p>
<hr>
<h4 id="31-spark支持故障恢复的方式？"><a href="#31-spark支持故障恢复的方式？" class="headerlink" title="31. spark支持故障恢复的方式？"></a><strong>31. spark支持故障恢复的方式？</strong></h4><ul>
<li>通过血缘关系lineage，当发生故障的时候通过血缘关系回溯，再执行一遍来一层一层恢复数据；</li>
<li>通过checkpoint()机制，将数据存储到持久化存储中来恢复数据。</li>
</ul>
<hr>
<h4 id="32-spark解决了hadoop的哪些问题？"><a href="#32-spark解决了hadoop的哪些问题？" class="headerlink" title="32. spark解决了hadoop的哪些问题？"></a><strong>32. spark解决了hadoop的哪些问题？</strong></h4><ol>
<li><ul>
<li>MR:抽象层次低，需要使用手工代码来完成程序编写，使用上难以上手；</li>
<li>Spark:Spark采用RDD计算模型，简单容易上手。</li>
</ul>
</li>
<li><ul>
<li>MR:只提供map和reduce两个操作，表达能力欠缺；</li>
<li>Spark:Spark采用更加丰富的算子模型，包括map、flatmap、groupbykey、reducebykey等；</li>
</ul>
</li>
<li><ul>
<li>MR:一个job只能包含map和reduce两个阶段，复杂的任务需要包含很多个job，这些job之间的管理以来需要开发者自己进行管理；</li>
<li>Spark:Spark中一个job可以包含多个转换操作，在调度时可以生成多个stage，而且如果多个map操作的分区不变，是可以放在同一个task里面去执行；</li>
</ul>
</li>
<li><ul>
<li>MR:中间结果存放在hdfs中；</li>
<li>Spark:Spark的中间结果一般存在内存中，只有当内存不够了，才会存入本地磁盘，而不是hdfs；</li>
</ul>
</li>
<li><ul>
<li>MR:只有等到所有的map task执行完毕后才能执行reduce task；</li>
<li>Spark:Spark中分区相同的转换构成流水线在一个task中执行，分区不同的需要进行shuffle操作，被划分成不同的stage需要等待前面的stage执行完才能执行。</li>
</ul>
</li>
<li><ul>
<li>MR:只适合batch批处理，时延高，对于交互式处理和实时处理支持不够；</li>
<li>Spark:Spark streaming可以将流拆成时间间隔的batch进行处理，实时计算。</li>
</ul>
</li>
<li><ul>
<li>MR:对于迭代式计算处理较差；</li>
<li>Spark:Spark将中间数据存放在内存中，提高迭代式计算性能。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="33-数据倾斜的产生和解决办法？"><a href="#33-数据倾斜的产生和解决办法？" class="headerlink" title="33. 数据倾斜的产生和解决办法？"></a><strong>33. 数据倾斜的产生和解决办法？</strong></h4><p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://mubu.com/doc/iK9RFqMfd0">Spark性能调优</a> 【六、数据倾斜解决方案】</em> <strong>密码：ibean.top，如果分享关闭请留言</strong></p>
<hr>
<h4 id="34-spark-实现高可用性：High-Availability？"><a href="#34-spark-实现高可用性：High-Availability？" class="headerlink" title="34. spark 实现高可用性：High Availability？"></a><strong>34. spark 实现高可用性：High Availability？</strong></h4><ul>
<li>基于文件系统的单点恢复(Single-Node Recovery with Local File System)</li>
<li>基于zookeeper的Standby Masters(Standby Masters with ZooKeeper)</li>
</ul>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/byrhuangqiang/p/3937654.html">Spark:Master High Availability（HA）高可用配置的2种实现</a></em></p>
<hr>
<h4 id="35-spark实际工作中，是怎么来根据任务量，判定需要多少资源的？"><a href="#35-spark实际工作中，是怎么来根据任务量，判定需要多少资源的？" class="headerlink" title="35. spark实际工作中，是怎么来根据任务量，判定需要多少资源的？"></a><strong>35. spark实际工作中，是怎么来根据任务量，判定需要多少资源的？</strong></h4><hr>
<h4 id="36-spark中怎么解决内存泄漏问题？"><a href="#36-spark中怎么解决内存泄漏问题？" class="headerlink" title="36. spark中怎么解决内存泄漏问题？"></a><strong>36. spark中怎么解决内存泄漏问题？</strong></h4><ol>
<li>driver端<ul>
<li>可以增大driver的内存参数：<code>spark.driver.memory</code> (default 1g)</li>
<li>DAGScheduler和Spark Context运行在Driver端，rdd的stage切分由driver完成；当程序算子过多可能会切分出大量stage，占用driver内存</li>
</ul>
</li>
<li>map过程产生大量对象<ul>
<li>单个map产生大量对象导致； </li>
<li>不增加内存的情况下，减少每个task的大小；可以在会产生大量对象的map操作之前调用repartition方法，分区成更小的块传入map</li>
</ul>
</li>
<li>数据不平衡导致内存溢出，解决方案与2相似，采用repartition的方式；</li>
<li>shuffle后内存溢出</li>
</ol>
<p><em><strong>参考链接</strong> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/Sunshine_2211468152/article/details/83050337">spark 如何防止内存溢出</a></em></p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://blog.ibean.eu.org">IBean</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://blog.ibean.eu.org/posts/45903.html">https://blog.ibean.eu.org/posts/45903.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://blog.ibean.eu.org" target="_blank">野生Java程序员兼美食博主</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a><a class="post-meta__tags" href="/tags/BigData/">BigData</a><a class="post-meta__tags" href="/tags/%E9%9D%A2%E8%AF%95/">面试</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/58315.html" title="大数据-Hive篇-面试题总结"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">大数据-Hive篇-面试题总结</div></div></a></div><div class="next-post pull-right"><a href="/posts/52460.html" title="Centos7离线安装Vim"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Centos7离线安装Vim</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/58315.html" title="大数据-Hive篇-面试题总结"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-08</div><div class="title">大数据-Hive篇-面试题总结</div></div></a></div><div><a href="/posts/64906.html" title="Spark2.0.2+Scala2.11.8环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-12-24</div><div class="title">Spark2.0.2+Scala2.11.8环境搭建</div></div></a></div><div><a href="/posts/2625.html" title="Hive1.2.2环境搭建（MariaDB版）【填坑】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-01-08</div><div class="title">Hive1.2.2环境搭建（MariaDB版）【填坑】</div></div></a></div><div><a href="/posts/4561.html" title="初入Storm开发遇到的问题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-03-08</div><div class="title">初入Storm开发遇到的问题</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">IBean</div><div class="author-info__description">野生Java程序员兼美食博主</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">32</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">22</div></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Spark%E7%9A%84Shuffle%E5%8E%9F%E7%90%86%E5%8F%8A%E8%B0%83%E4%BC%98%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">1. Spark的Shuffle原理及调优？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-hadoop%E5%92%8Cspark%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">2. hadoop和spark使用场景？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-spark%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%AE%95%E6%9C%BA%E8%BF%85%E9%80%9F%E6%81%A2%E5%A4%8D"><span class="toc-number">3.</span> <span class="toc-text">3. spark如何保证宕机迅速恢复?</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-hadoop%E5%92%8Cspark%E7%9A%84%E7%9B%B8%E5%90%8C%E7%82%B9%E5%92%8C%E4%B8%8D%E5%90%8C%E7%82%B9%EF%BC%9F"><span class="toc-number">4.</span> <span class="toc-text">4. hadoop和spark的相同点和不同点？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-RDD%E6%8C%81%E4%B9%85%E5%8C%96%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">5.</span> <span class="toc-text">5. RDD持久化原理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-checkpoint%E6%A3%80%E6%9F%A5%E7%82%B9%E6%9C%BA%E5%88%B6%EF%BC%9F"><span class="toc-number">6.</span> <span class="toc-text">6. checkpoint检查点机制？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7-checkpoint%E5%92%8C%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">7.</span> <span class="toc-text">7. checkpoint和持久化机制的区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8-Spark-Streaming%E5%92%8CStorm%E6%9C%89%E4%BD%95%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">8.</span> <span class="toc-text">8. Spark Streaming和Storm有何区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-RDD%E6%9C%BA%E5%88%B6%EF%BC%9F"><span class="toc-number">9.</span> <span class="toc-text">9. RDD机制？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10-Spark-streaming%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">10.</span> <span class="toc-text">10. Spark streaming以及基本工作原理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11-DStream%E4%BB%A5%E5%8F%8A%E5%9F%BA%E6%9C%AC%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">11.</span> <span class="toc-text">11. DStream以及基本工作原理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-spark%E6%9C%89%E5%93%AA%E4%BA%9B%E7%BB%84%E4%BB%B6%EF%BC%9F"><span class="toc-number">12.</span> <span class="toc-text">12. spark有哪些组件？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#13-spark%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%9F"><span class="toc-number">13.</span> <span class="toc-text">13. spark工作机制？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#14-Spark%E5%B7%A5%E4%BD%9C%E7%9A%84%E4%B8%80%E4%B8%AA%E6%B5%81%E7%A8%8B%EF%BC%9F"><span class="toc-number">14.</span> <span class="toc-text">14. Spark工作的一个流程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#15-spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">15.</span> <span class="toc-text">15. spark核心编程原理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#16-spark%E5%9F%BA%E6%9C%AC%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">16.</span> <span class="toc-text">16. spark基本工作原理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#17-spark%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">17.</span> <span class="toc-text">17. spark性能优化有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#18-updateStateByKey%E8%AF%A6%E8%A7%A3%EF%BC%9F"><span class="toc-number">18.</span> <span class="toc-text">18. updateStateByKey详解？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#19-%E5%AE%BD%E4%BE%9D%E8%B5%96%E5%92%8C%E7%AA%84%E4%BE%9D%E8%B5%96%EF%BC%9F"><span class="toc-number">19.</span> <span class="toc-text">19. 宽依赖和窄依赖？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#20-spark-streaming%E4%B8%AD%E6%9C%89%E7%8A%B6%E6%80%81%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C%EF%BC%9F"><span class="toc-number">20.</span> <span class="toc-text">20. spark streaming中有状态转化操作？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#21-spark%E5%B8%B8%E7%94%A8%E7%9A%84%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%EF%BC%9F"><span class="toc-number">21.</span> <span class="toc-text">21. spark常用的计算框架？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#22-spark%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">22.</span> <span class="toc-text">22. spark整体架构？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#23-Spark%E7%9A%84%E7%89%B9%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">23.</span> <span class="toc-text">23. Spark的特点是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#24-%E6%90%AD%E5%BB%BAspark%E9%9B%86%E7%BE%A4%E6%AD%A5%E9%AA%A4%EF%BC%9F"><span class="toc-number">24.</span> <span class="toc-text">24. 搭建spark集群步骤？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#25-Spark%E7%9A%84%E4%B8%89%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%A8%A1%E5%BC%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">25.</span> <span class="toc-text">25. Spark的三种提交模式是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#26-spark%E5%86%85%E6%A0%B8%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">26.</span> <span class="toc-text">26. spark内核架构原理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#27-Spark-yarn-cluster%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">27.</span> <span class="toc-text">27. Spark yarn-cluster架构？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#28-Spark-yarn-client%E6%9E%B6%E6%9E%84%EF%BC%9F"><span class="toc-number">28.</span> <span class="toc-text">28. Spark yarn-client架构？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#29-SparkContext%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8E%9F%E7%90%86%EF%BC%9F"><span class="toc-number">29.</span> <span class="toc-text">29. SparkContext初始化原理？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#30-Spark%E4%B8%BB%E5%A4%87%E5%88%87%E6%8D%A2%E6%9C%BA%E5%88%B6%E5%8E%9F%E7%90%86%E5%89%96%E6%9E%90%EF%BC%9F"><span class="toc-number">30.</span> <span class="toc-text">30. Spark主备切换机制原理剖析？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#31-spark%E6%94%AF%E6%8C%81%E6%95%85%E9%9A%9C%E6%81%A2%E5%A4%8D%E7%9A%84%E6%96%B9%E5%BC%8F%EF%BC%9F"><span class="toc-number">31.</span> <span class="toc-text">31. spark支持故障恢复的方式？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#32-spark%E8%A7%A3%E5%86%B3%E4%BA%86hadoop%E7%9A%84%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">32.</span> <span class="toc-text">32. spark解决了hadoop的哪些问题？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#33-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%9A%84%E4%BA%A7%E7%94%9F%E5%92%8C%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%EF%BC%9F"><span class="toc-number">33.</span> <span class="toc-text">33. 数据倾斜的产生和解决办法？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#34-spark-%E5%AE%9E%E7%8E%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E6%80%A7%EF%BC%9AHigh-Availability%EF%BC%9F"><span class="toc-number">34.</span> <span class="toc-text">34. spark 实现高可用性：High Availability？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#35-spark%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E4%B8%AD%EF%BC%8C%E6%98%AF%E6%80%8E%E4%B9%88%E6%9D%A5%E6%A0%B9%E6%8D%AE%E4%BB%BB%E5%8A%A1%E9%87%8F%EF%BC%8C%E5%88%A4%E5%AE%9A%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E8%B5%84%E6%BA%90%E7%9A%84%EF%BC%9F"><span class="toc-number">35.</span> <span class="toc-text">35. spark实际工作中，是怎么来根据任务量，判定需要多少资源的？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#36-spark%E4%B8%AD%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">36.</span> <span class="toc-text">36. spark中怎么解决内存泄漏问题？</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/cc3e20e9.html" title="MongoDB初见-基础使用">MongoDB初见-基础使用</a><time datetime="2021-03-01T03:20:42.000Z" title="发表于 2021-03-01 11:20:42">2021-03-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/372e50b0.html" title="MongoDB初见">MongoDB初见</a><time datetime="2021-02-23T00:24:10.000Z" title="发表于 2021-02-23 08:24:10">2021-02-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/27176.html" title="MongoDB自定义类型转换器">MongoDB自定义类型转换器</a><time datetime="2021-02-06T09:18:48.000Z" title="发表于 2021-02-06 17:18:48">2021-02-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/37627.html" title="Terminal改造从Powershell开始">Terminal改造从Powershell开始</a><time datetime="2020-08-23T14:49:50.000Z" title="发表于 2020-08-23 22:49:50">2020-08-23</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/9258.html" title="Docker Java 常用开发环境">Docker Java 常用开发环境</a><time datetime="2020-08-22T08:30:44.000Z" title="发表于 2020-08-22 16:30:44">2020-08-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By IBean</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'PqCBXk1MjHUPGBpJIFCDiXwD-MdYXbMMI',
      appKey: 'QDY3F7RxV4DcMZAePf68P2bn',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>